# VLASH Remote Inference Latency Benchmark Configuration for PI05
# Measures single inference latency using real dataset samples via remote server

# Benchmark type
type: inference_latency

# Policy configuration
policy:
  type: pi05
  device: cuda
  dtype: bfloat16
  
  # Enable torch.compile for optimized inference
  compile_model: true
  
  # Enable attention/MLP fusion for faster inference
  fuse_qkv: true
  fuse_gate_up: true

# Dataset configuration
dataset:
  repo_id: lerobot/pusht # Change to your dataset
  streaming: false


# Number of samples to measure 
num_samples: 16

# Warmup iterations 
warmup_steps: 10

# Batch size (typically 1 for real-time inference benchmarking)
batch_size: 1

num_workers: 0

# Random seed for reproducibility
seed: 42

# Required by TrainPipelineConfig but not used for benchmarking
output_dir: outputs/benchmarks/inference_latency/pi05

# Remote Inference Configuration
remote_inference:
  # Server address (format: "host:port")
  # Use "localhost:50051" for local testing
  # Use actual IP for remote server, e.g., "192.168.1.100:50051"
  server_address: <YOUR_SERVER_IP>:<YOUR_SERVER_PORT>
  
  # Maximum message size for gRPC (bytes)
  # Increase if working with high-resolution images or large observations
  max_message_size: 4194304  # 4 MB
  
  # Enable gRPC automatic retries on network failures
  enable_retries: true
  
  # Maximum retry attempts
  max_attempts: 5
