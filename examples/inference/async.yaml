# VLASH Inference Configuration for PI05 with Async Inference
# This config enables async inference for faster real-time control

# Robot configuration
robot:
  type: so101_follower
  port: /dev/ttyACM2  # Update to match your robot's port
  id: my_awesome_follower_arm
  cameras:
    wrist:
      type: opencv
      index_or_path: 0  # Update to match your camera
      width: 640
      height: 480
      fps: 30

# Policy configuration
policy:
# Fill in the path to the policy checkpoint
  path: <path to the policy checkpoint>
  n_action_steps: 32  # Actions executed per inference
  compile_model: true  # Required for async inference
  device: cuda
  fuse_qkv: true
  fuse_gate_up: true

# Task description passed to the policy
single_task: <task description>

# Control parameters
fps: 30  # Control loop frequency (Hz)
control_time_s: 600  # Total runtime (seconds)

# Visualization
display_data: false  # Camera feeds via rerun
play_sounds: false  # Audio feedback

# Action quantization: execute action every N control steps
# action_quant_ratio=2 means policy infers every step but robot moves every 2nd
action_quant_ratio: 2

# Async inference: start next chunk N steps before current chunk ends
# With n_action_steps=32 and inference_overlap_steps=4:
#   - Current chunk: actions [0..31]
#   - At action 28, start inference for next chunk
#   - Next chunk ready by action 32
# Higher overlap = more GPU time, but uses slightly older observations
inference_overlap_steps: 4
